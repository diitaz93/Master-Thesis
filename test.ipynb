{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from operator import itemgetter\n",
    "from itertools import combinations, chain\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import psutil\n",
    "from decagon.deep.optimizer import DecagonOptimizer\n",
    "from decagon.deep.model import DecagonModel\n",
    "from decagon.deep.minibatch import EdgeMinibatchIterator\n",
    "from decagon.utility import rank_metrics, preprocessing\n",
    "from data.load_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psutil BEGIN\n",
    "start = time.time() #in seconds\n",
    "pid = os.getpid()\n",
    "ps= psutil.Process(pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.111137151718\n"
     ]
    }
   ],
   "source": [
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on GPU\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_scores(edges_pos, edges_neg, edge_type):\n",
    "    feed_dict.update({placeholders['dropout']: 0})\n",
    "    feed_dict.update({placeholders['batch_edge_type_idx']: minibatch.edge_type2idx[edge_type]})\n",
    "    feed_dict.update({placeholders['batch_row_edge_type']: edge_type[0]})\n",
    "    feed_dict.update({placeholders['batch_col_edge_type']: edge_type[1]})\n",
    "    rec = sess.run(opt.predictions, feed_dict=feed_dict)\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1. / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    preds = []\n",
    "    actual = []\n",
    "    predicted = []\n",
    "    edge_ind = 0\n",
    "    for u, v in edges_pos[edge_type[:2]][edge_type[2]]:\n",
    "        score = sigmoid(rec[u, v])\n",
    "        preds.append(score)\n",
    "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 1, 'Problem 1'\n",
    "\n",
    "        actual.append(edge_ind)\n",
    "        predicted.append((score, edge_ind))\n",
    "        edge_ind += 1\n",
    "\n",
    "    preds_neg = []\n",
    "    for u, v in edges_neg[edge_type[:2]][edge_type[2]]:\n",
    "        score = sigmoid(rec[u, v])\n",
    "        preds_neg.append(score)\n",
    "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 0, 'Problem 0'\n",
    "\n",
    "        predicted.append((score, edge_ind))\n",
    "        edge_ind += 1\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    preds_all = np.nan_to_num(preds_all)\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "    predicted = list(zip(*sorted(predicted, reverse=True, key=itemgetter(0))))[1]\n",
    "\n",
    "    roc_sc = metrics.roc_auc_score(labels_all, preds_all)\n",
    "    aupr_sc = metrics.average_precision_score(labels_all, preds_all)\n",
    "    apk_sc = rank_metrics.apk(actual, predicted, k=50)\n",
    "\n",
    "    return roc_sc, aupr_sc, apk_sc\n",
    "\n",
    "\n",
    "def construct_placeholders(edge_types):\n",
    "    placeholders = {\n",
    "        'batch': tf.placeholder(tf.int32, name='batch'),\n",
    "        'batch_edge_type_idx': tf.placeholder(tf.int32, shape=(), name='batch_edge_type_idx'),\n",
    "        'batch_row_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_row_edge_type'),\n",
    "        'batch_col_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_col_edge_type'),\n",
    "        'degrees': tf.placeholder(tf.int32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    }\n",
    "    placeholders.update({\n",
    "        'adj_mats_%d,%d,%d' % (i, j, k): tf.sparse_placeholder(tf.float32)\n",
    "        for i, j in edge_types for k in range(edge_types[i,j])})\n",
    "    placeholders.update({\n",
    "        'feat_%d' % i: tf.sparse_placeholder(tf.float32)\n",
    "        for i, _ in edge_types})\n",
    "    return placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data \n",
    "PF can also be imported with\n",
    "``` \n",
    "PF = np.genfromtxt('data/clean_data/genes_mini.csv', delimiter=',',dtype='float64',skip_header=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ppi interactions: 60723\n",
      "Number of genes: 9388\n",
      "Number of individual side effects:  9238\n",
      "Number fo DTI interactions: 9643\n",
      "Number of genes in DTI: 635\n",
      "Number of drugs in DTI: 164\n",
      "Drug combinations: 4242 Side effects: 3\n",
      "Drug-drug interactions: 4531\n",
      "Number of drugs:  349\n"
     ]
    }
   ],
   "source": [
    "# Loading Gene data (PPI)\n",
    "ppi, gene2idx = load_ppi(fname='data/clean_data/ppi_mini.csv')\n",
    "ppi_adj = nx.adjacency_matrix(ppi)\n",
    "ppi_degrees = np.array(ppi_adj.sum(axis=0)).squeeze() \n",
    "ppi_genes = ppi.number_of_nodes() # Number of genes (nodes)\n",
    "# Loading individual side effects\n",
    "stitch2se, semono2name, semono2idx = load_mono_se(fname='data/clean_data/mono_mini.csv')\n",
    "n_semono = len(semono2name)\n",
    "print('Number of individual side effects: ', n_semono)\n",
    "# Loading Target data (DTI)\n",
    "stitch2proteins = load_targets(fname='data/clean_data/targets_mini.csv')\n",
    "dti_drugs = len(pd.unique(stitch2proteins.keys()))\n",
    "dti_genes = len(set(chain.from_iterable(stitch2proteins.itervalues())))\n",
    "print('Number of genes in DTI:', dti_genes)\n",
    "print('Number of drugs in DTI:', dti_drugs)\n",
    "# Loading Drug data (DDI)\n",
    "combo2stitch, combo2se, secombo2name, drug2idx = load_combo_se(fname='data/clean_data/combo_mini.csv')\n",
    "# Loading Side effect data (features)\n",
    "stitch2se, semono2name, semono2idx = load_mono_se(fname='data/clean_data/mono_mini.csv')\n",
    "# Loading protein features\n",
    "PF = pd.read_csv('data/clean_data/genes_mini.csv', sep=',',header=None).to_numpy()\n",
    "ddi_drugs = len(drug2idx)\n",
    "print('Number of drugs: ', ddi_drugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drug-target adjacency matrix\n",
    "dti_adj = np.zeros([ppi_genes,ddi_drugs],dtype=int)\n",
    "for drug in drug2idx.keys():\n",
    "    for gene in stitch2proteins[drug]:\n",
    "        if gene==set():\n",
    "            continue\n",
    "        else:\n",
    "            idp = gene2idx[str(gene)]\n",
    "            idd = drug2idx[drug]\n",
    "            dti_adj[idp,idd] = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dti_adj = sp.csr_matrix(dti_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDI adjacency matrix\n",
    "ddi_adj_list = []\n",
    "for se in secombo2name.keys():\n",
    "    m = np.zeros([ddi_drugs,ddi_drugs],dtype=int)\n",
    "    for pair in combo2se.keys():\n",
    "        if se in combo2se[pair]:\n",
    "            d1,d2 = combo2stitch[pair]\n",
    "            m[drug2idx[d1],drug2idx[d2]] = m[drug2idx[d2],drug2idx[d1]] = 1\n",
    "    ddi_adj_list.append(sp.csr_matrix(m))    \n",
    "ddi_degrees_list = [np.array(drug_adj.sum(axis=0)).squeeze() for drug_adj in ddi_adj_list]\n",
    "\n",
    "adj_mats_orig = {\n",
    "    (0, 0): [ppi_adj, ppi_adj.transpose(copy=True)],\n",
    "    (0, 1): [dti_adj],\n",
    "    (1, 0): [dti_adj.transpose(copy=True)],\n",
    "    (1, 1): ddi_adj_list + [x.transpose(copy=True) for x in ddi_adj_list],\n",
    "}\n",
    "degrees = {\n",
    "    0: [ppi_degrees, ppi_degrees],\n",
    "    1: ddi_degrees_list + ddi_degrees_list, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featureless (genes)\n",
    "gene_feat = sp.identity(ppi_genes)\n",
    "#gene_feat = sp.coo_matrix(PF)\n",
    "gene_nonzero_feat, gene_num_feat = 2*[gene_feat.shape[1]]\n",
    "gene_feat = preprocessing.sparse_to_tuple(gene_feat.tocoo())\n",
    "# features (drugs)\n",
    "oh_feat = np.zeros([ddi_drugs,n_semono], dtype=int)\n",
    "for drug in drug2idx.keys():\n",
    "    for se in stitch2se[drug]:\n",
    "        did = drug2idx[drug]\n",
    "        seid = semono2idx[se]\n",
    "        oh_feat[did,seid] = 1\n",
    "drug_feat = sp.csr_matrix(oh_feat)\n",
    "drug_nonzero_feat = n_semono\n",
    "drug_num_feat = n_semono\n",
    "drug_feat = preprocessing.sparse_to_tuple(drug_feat.tocoo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge types: 10\n"
     ]
    }
   ],
   "source": [
    "# data representation\n",
    "num_feat = {\n",
    "    0: gene_num_feat,\n",
    "    1: drug_num_feat,\n",
    "}\n",
    "nonzero_feat = {\n",
    "    0: gene_nonzero_feat,\n",
    "    1: drug_nonzero_feat,\n",
    "}\n",
    "feat = {\n",
    "    0: gene_feat,\n",
    "    1: drug_feat,\n",
    "}\n",
    "# Dictionary with the shape of all the matrices of the dictionary adj_mats_orig\n",
    "edge_type2dim = {k: [adj.shape for adj in adjs] for k, adjs in adj_mats_orig.items()}\n",
    "edge_type2decoder = {\n",
    "    (0, 0): 'bilinear',\n",
    "    (0, 1): 'bilinear',\n",
    "    (1, 0): 'bilinear',\n",
    "    (1, 1): 'dedicom',\n",
    "}\n",
    "#Dictionary with the number of matrices for each entry of adj_mats_orig\n",
    "edge_types = {k: len(v) for k, v in adj_mats_orig.items()}\n",
    "num_edge_types = sum(edge_types.values())\n",
    "print(\"Edge types:\", \"%d\" % num_edge_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test_size = 0.05\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('neg_sample_size', 1, 'Negative sample size.')\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 10, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 64, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\n",
    "flags.DEFINE_float('weight_decay', 0, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_float('dropout', 0.1, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('max_margin', 0.1, 'Max margin parameter in hinge loss')\n",
    "flags.DEFINE_integer('batch_size', 512, 'minibatch size.')\n",
    "flags.DEFINE_boolean('bias', True, 'Bias term.')\n",
    "# Important -- Do not evaluate/print validation performance every iteration as it can take\n",
    "# substantial amount of time\n",
    "PRINT_PROGRESS_EVERY = 150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining placeholders\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining placeholders\")\n",
    "placeholders = construct_placeholders(edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MACHETAZO!! Soluciona el bug de Jupyter con tensorflow que proporciona un flag -f\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create minibatch iterator, model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create minibatch iterator\n",
      "Minibatch edge type: (0, 1, 0)\n",
      "Constructing test edges= 0000/0482\n",
      "Constructing val edges= 0000/0482\n",
      "Train edges= 8679\n",
      "Val edges= 0482\n",
      "Test edges= 0482\n",
      "Minibatch edge type: (1, 0, 0)\n",
      "Constructing test edges= 0000/0482\n",
      "Constructing val edges= 0000/0482\n",
      "Train edges= 8679\n",
      "Val edges= 0482\n",
      "Test edges= 0482\n",
      "Minibatch edge type: (0, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decagon/deep/minibatch.py:67: RuntimeWarning: divide by zero encountered in power\n",
      "  rowdegree_mat_inv = sp.diags(np.nan_to_num(np.power(rowsum, -0.5)).flatten())\n",
      "decagon/deep/minibatch.py:68: RuntimeWarning: divide by zero encountered in power\n",
      "  coldegree_mat_inv = sp.diags(np.nan_to_num(np.power(colsum, -0.5)).flatten())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing test edges= 0000/6072\n",
      "Constructing test edges= 1000/6072\n",
      "Constructing test edges= 2000/6072\n",
      "Constructing test edges= 3000/6072\n",
      "Constructing test edges= 4000/6072\n",
      "Constructing test edges= 5000/6072\n",
      "Constructing test edges= 6000/6072\n",
      "Constructing val edges= 0000/6072\n",
      "Constructing val edges= 1000/6072\n",
      "Constructing val edges= 2000/6072\n",
      "Constructing val edges= 3000/6072\n",
      "Constructing val edges= 4000/6072\n",
      "Constructing val edges= 5000/6072\n",
      "Constructing val edges= 6000/6072\n",
      "Train edges= 109302\n",
      "Val edges= 6072\n",
      "Test edges= 6072\n",
      "Minibatch edge type: (0, 0, 1)\n",
      "Constructing test edges= 0000/6072\n",
      "Constructing test edges= 1000/6072\n",
      "Constructing test edges= 2000/6072\n",
      "Constructing test edges= 3000/6072\n",
      "Constructing test edges= 4000/6072\n",
      "Constructing test edges= 5000/6072\n",
      "Constructing test edges= 6000/6072\n",
      "Constructing val edges= 0000/6072\n",
      "Constructing val edges= 1000/6072\n",
      "Constructing val edges= 2000/6072\n",
      "Constructing val edges= 3000/6072\n",
      "Constructing val edges= 4000/6072\n",
      "Constructing val edges= 5000/6072\n",
      "Constructing val edges= 6000/6072\n",
      "Train edges= 109302\n",
      "Val edges= 6072\n",
      "Test edges= 6072\n",
      "Minibatch edge type: (1, 1, 0)\n",
      "Constructing test edges= 0000/0092\n",
      "Constructing val edges= 0000/0092\n",
      "Train edges= 1666\n",
      "Val edges= 0092\n",
      "Test edges= 0092\n",
      "Minibatch edge type: (1, 1, 1)\n",
      "Constructing test edges= 0000/0229\n",
      "Constructing val edges= 0000/0229\n",
      "Train edges= 4134\n",
      "Val edges= 0229\n",
      "Test edges= 0229\n",
      "Minibatch edge type: (1, 1, 2)\n",
      "Constructing test edges= 0000/0131\n",
      "Constructing val edges= 0000/0131\n",
      "Train edges= 2358\n",
      "Val edges= 0131\n",
      "Test edges= 0131\n",
      "Minibatch edge type: (1, 1, 3)\n",
      "Constructing test edges= 0000/0092\n",
      "Constructing val edges= 0000/0092\n",
      "Train edges= 1666\n",
      "Val edges= 0092\n",
      "Test edges= 0092\n",
      "Minibatch edge type: (1, 1, 4)\n",
      "Constructing test edges= 0000/0229\n",
      "Constructing val edges= 0000/0229\n",
      "Train edges= 4134\n",
      "Val edges= 0229\n",
      "Test edges= 0229\n",
      "Minibatch edge type: (1, 1, 5)\n",
      "Constructing test edges= 0000/0131\n",
      "Constructing test edges= 0000/0131\n",
      "Constructing val edges= 0000/0131\n",
      "Train edges= 2358\n",
      "Val edges= 0131\n",
      "Test edges= 0131\n"
     ]
    }
   ],
   "source": [
    "print(\"Create minibatch iterator\")\n",
    "minibatch = EdgeMinibatchIterator(\n",
    "    adj_mats=adj_mats_orig,\n",
    "    feat=feat,\n",
    "    edge_types=edge_types,\n",
    "    batch_size=FLAGS.batch_size,\n",
    "    val_test_size=val_test_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create model\n",
      "WARNING:tensorflow:From decagon/deep/layers.py:93: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "print(\"Create model\")\n",
    "model = DecagonModel(\n",
    "    placeholders=placeholders,\n",
    "    num_feat=num_feat,\n",
    "    nonzero_feat=nonzero_feat,\n",
    "    edge_types=edge_types,\n",
    "    decoders=edge_type2decoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create optimizer\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "print(\"Create optimizer\")\n",
    "with tf.name_scope('optimizer'):\n",
    "    opt = DecagonOptimizer(\n",
    "        embeddings=model.embeddings,\n",
    "        latent_inters=model.latent_inters,\n",
    "        latent_varies=model.latent_varies,\n",
    "        degrees=degrees,\n",
    "        edge_types=edge_types,\n",
    "        edge_type2dim=edge_type2dim,\n",
    "        placeholders=placeholders,\n",
    "        batch_size=FLAGS.batch_size,\n",
    "        margin=FLAGS.max_margin\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize session\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialize session\")\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "feed_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model\n",
      "Epoch: 0001 Iter: 0001 Edge: 0002 train_loss= 718.66821 val_roc= 0.68140 val_auprc= 0.71901 val_apk= 0.96097 time= 1.62027\n",
      "Epoch: 0001 Iter: 0151 Edge: 0001 train_loss= 281.62457 val_roc= 0.88315 val_auprc= 0.91108 val_apk= 1.00000 time= 0.27752\n",
      "Epoch: 0001 Iter: 0301 Edge: 0002 train_loss= 470.39966 val_roc= 0.93373 val_auprc= 0.93435 val_apk= 0.97791 time= 0.74219\n",
      "Epoch: 0001 Iter: 0451 Edge: 0001 train_loss= 268.89493 val_roc= 0.91844 val_auprc= 0.93087 val_apk= 1.00000 time= 0.26680\n",
      "Epoch: 0001 Iter: 0601 Edge: 0002 train_loss= 426.59525 val_roc= 0.96505 val_auprc= 0.96101 val_apk= 1.00000 time= 0.87615\n",
      "Epoch: 0001 Iter: 0751 Edge: 0001 train_loss= 237.35733 val_roc= 0.92736 val_auprc= 0.93740 val_apk= 1.00000 time= 0.28168\n",
      "Epoch: 0001 Iter: 0901 Edge: 0002 train_loss= 385.06189 val_roc= 0.97046 val_auprc= 0.96536 val_apk= 0.93001 time= 0.74475\n",
      "Epoch: 0002 Iter: 0001 Edge: 0002 train_loss= 354.14807 val_roc= 0.97078 val_auprc= 0.96651 val_apk= 0.94435 time= 0.79135\n",
      "Epoch: 0002 Iter: 0151 Edge: 0001 train_loss= 243.35712 val_roc= 0.93420 val_auprc= 0.94191 val_apk= 1.00000 time= 0.27909\n",
      "Epoch: 0002 Iter: 0301 Edge: 0002 train_loss= 361.56555 val_roc= 0.97632 val_auprc= 0.97392 val_apk= 1.00000 time= 0.77451\n",
      "Epoch: 0002 Iter: 0451 Edge: 0001 train_loss= 240.52600 val_roc= 0.93580 val_auprc= 0.94377 val_apk= 1.00000 time= 0.28090\n",
      "Epoch: 0002 Iter: 0601 Edge: 0002 train_loss= 397.01031 val_roc= 0.97629 val_auprc= 0.97387 val_apk= 0.95041 time= 0.79328\n",
      "Epoch: 0002 Iter: 0751 Edge: 0001 train_loss= 233.75481 val_roc= 0.96762 val_auprc= 0.96983 val_apk= 1.00000 time= 0.28563\n",
      "Epoch: 0002 Iter: 0901 Edge: 0002 train_loss= 322.55493 val_roc= 0.97990 val_auprc= 0.97868 val_apk= 1.00000 time= 0.77893\n",
      "Epoch: 0003 Iter: 0001 Edge: 0002 train_loss= 307.31250 val_roc= 0.97991 val_auprc= 0.97878 val_apk= 1.00000 time= 0.79873\n",
      "Epoch: 0003 Iter: 0151 Edge: 0001 train_loss= 224.01662 val_roc= 0.95575 val_auprc= 0.95840 val_apk= 1.00000 time= 0.29204\n",
      "Epoch: 0003 Iter: 0301 Edge: 0002 train_loss= 317.58517 val_roc= 0.98072 val_auprc= 0.97935 val_apk= 1.00000 time= 0.82122\n",
      "Epoch: 0003 Iter: 0451 Edge: 0001 train_loss= 217.37511 val_roc= 0.96331 val_auprc= 0.96632 val_apk= 1.00000 time= 0.27139\n",
      "Epoch: 0003 Iter: 0601 Edge: 0002 train_loss= 353.72479 val_roc= 0.98115 val_auprc= 0.98093 val_apk= 1.00000 time= 0.86970\n",
      "Epoch: 0003 Iter: 0751 Edge: 0001 train_loss= 217.44106 val_roc= 0.95355 val_auprc= 0.95599 val_apk= 1.00000 time= 0.30322\n",
      "Epoch: 0003 Iter: 0901 Edge: 0002 train_loss= 336.57056 val_roc= 0.98353 val_auprc= 0.98345 val_apk= 1.00000 time= 0.79945\n",
      "Epoch: 0004 Iter: 0001 Edge: 0002 train_loss= 295.90457 val_roc= 0.98362 val_auprc= 0.98294 val_apk= 1.00000 time= 0.79620\n",
      "Epoch: 0004 Iter: 0151 Edge: 0001 train_loss= 237.19489 val_roc= 0.94933 val_auprc= 0.95132 val_apk= 1.00000 time= 0.27180\n",
      "Epoch: 0004 Iter: 0301 Edge: 0002 train_loss= 313.49670 val_roc= 0.98273 val_auprc= 0.98187 val_apk= 1.00000 time= 0.77009\n",
      "Epoch: 0004 Iter: 0451 Edge: 0001 train_loss= 234.83401 val_roc= 0.94906 val_auprc= 0.95049 val_apk= 1.00000 time= 0.28351\n",
      "Epoch: 0004 Iter: 0601 Edge: 0002 train_loss= 312.54358 val_roc= 0.98368 val_auprc= 0.98208 val_apk= 1.00000 time= 0.93144\n",
      "Epoch: 0004 Iter: 0751 Edge: 0001 train_loss= 242.66177 val_roc= 0.95063 val_auprc= 0.95315 val_apk= 1.00000 time= 0.29239\n",
      "Epoch: 0004 Iter: 0901 Edge: 0002 train_loss= 311.00262 val_roc= 0.98529 val_auprc= 0.98441 val_apk= 1.00000 time= 0.78574\n",
      "Epoch: 0005 Iter: 0001 Edge: 0002 train_loss= 309.80243 val_roc= 0.98500 val_auprc= 0.98386 val_apk= 1.00000 time= 0.76214\n",
      "Epoch: 0005 Iter: 0151 Edge: 0001 train_loss= 230.38394 val_roc= 0.95530 val_auprc= 0.95420 val_apk= 1.00000 time= 0.28946\n",
      "Epoch: 0005 Iter: 0301 Edge: 0002 train_loss= 253.74100 val_roc= 0.98556 val_auprc= 0.98494 val_apk= 1.00000 time= 0.90638\n",
      "Epoch: 0005 Iter: 0451 Edge: 0001 train_loss= 248.98019 val_roc= 0.94459 val_auprc= 0.94426 val_apk= 1.00000 time= 0.30830\n",
      "Epoch: 0005 Iter: 0601 Edge: 0002 train_loss= 280.04865 val_roc= 0.98601 val_auprc= 0.98470 val_apk= 1.00000 time= 0.77946\n",
      "Epoch: 0005 Iter: 0751 Edge: 0001 train_loss= 199.47522 val_roc= 0.96261 val_auprc= 0.96025 val_apk= 1.00000 time= 0.28096\n",
      "Epoch: 0005 Iter: 0901 Edge: 0002 train_loss= 279.55646 val_roc= 0.98475 val_auprc= 0.98326 val_apk= 1.00000 time= 0.80974\n",
      "Epoch: 0006 Iter: 0001 Edge: 0002 train_loss= 269.71851 val_roc= 0.98603 val_auprc= 0.98435 val_apk= 1.00000 time= 0.78235\n",
      "Epoch: 0006 Iter: 0151 Edge: 0001 train_loss= 215.31427 val_roc= 0.96668 val_auprc= 0.96372 val_apk= 0.97457 time= 0.27461\n",
      "Epoch: 0006 Iter: 0301 Edge: 0002 train_loss= 250.29408 val_roc= 0.98634 val_auprc= 0.98494 val_apk= 1.00000 time= 0.80058\n",
      "Epoch: 0006 Iter: 0451 Edge: 0001 train_loss= 216.57117 val_roc= 0.95632 val_auprc= 0.95359 val_apk= 0.97559 time= 0.27779\n",
      "Epoch: 0006 Iter: 0601 Edge: 0002 train_loss= 307.46936 val_roc= 0.98776 val_auprc= 0.98676 val_apk= 1.00000 time= 0.79244\n",
      "Epoch: 0006 Iter: 0751 Edge: 0001 train_loss= 224.48471 val_roc= 0.95613 val_auprc= 0.95339 val_apk= 0.97607 time= 0.28064\n",
      "Epoch: 0006 Iter: 0901 Edge: 0002 train_loss= 264.73151 val_roc= 0.98813 val_auprc= 0.98702 val_apk= 1.00000 time= 0.80973\n",
      "Epoch: 0007 Iter: 0001 Edge: 0002 train_loss= 272.78574 val_roc= 0.98781 val_auprc= 0.98746 val_apk= 1.00000 time= 0.78197\n",
      "Epoch: 0007 Iter: 0151 Edge: 0001 train_loss= 217.82857 val_roc= 0.95859 val_auprc= 0.95485 val_apk= 0.96856 time= 0.27747\n",
      "Epoch: 0007 Iter: 0301 Edge: 0002 train_loss= 275.98978 val_roc= 0.98725 val_auprc= 0.98648 val_apk= 1.00000 time= 0.74877\n",
      "Epoch: 0007 Iter: 0451 Edge: 0001 train_loss= 247.94466 val_roc= 0.94399 val_auprc= 0.94175 val_apk= 0.97238 time= 0.27779\n",
      "Epoch: 0007 Iter: 0601 Edge: 0002 train_loss= 282.31366 val_roc= 0.98869 val_auprc= 0.98805 val_apk= 1.00000 time= 0.78351\n",
      "Epoch: 0007 Iter: 0751 Edge: 0001 train_loss= 202.42484 val_roc= 0.95457 val_auprc= 0.95169 val_apk= 0.97457 time= 0.27488\n",
      "Epoch: 0007 Iter: 0901 Edge: 0002 train_loss= 311.48740 val_roc= 0.98891 val_auprc= 0.98791 val_apk= 1.00000 time= 0.78299\n",
      "Epoch: 0008 Iter: 0001 Edge: 0002 train_loss= 281.79553 val_roc= 0.98843 val_auprc= 0.98717 val_apk= 1.00000 time= 0.77308\n",
      "Epoch: 0008 Iter: 0151 Edge: 0001 train_loss= 234.22357 val_roc= 0.96232 val_auprc= 0.95776 val_apk= 0.96710 time= 0.28440\n",
      "Epoch: 0008 Iter: 0301 Edge: 0002 train_loss= 271.66364 val_roc= 0.98924 val_auprc= 0.98834 val_apk= 1.00000 time= 0.76819\n",
      "Epoch: 0008 Iter: 0451 Edge: 0001 train_loss= 247.86450 val_roc= 0.94453 val_auprc= 0.94126 val_apk= 0.96383 time= 0.28091\n",
      "Epoch: 0008 Iter: 0601 Edge: 0002 train_loss= 246.93094 val_roc= 0.98749 val_auprc= 0.98629 val_apk= 1.00000 time= 0.79110\n",
      "Epoch: 0008 Iter: 0751 Edge: 0001 train_loss= 213.40364 val_roc= 0.95313 val_auprc= 0.94908 val_apk= 0.96710 time= 0.27412\n",
      "Epoch: 0008 Iter: 0901 Edge: 0002 train_loss= 263.94394 val_roc= 0.98912 val_auprc= 0.98712 val_apk= 1.00000 time= 0.75153\n",
      "Epoch: 0009 Iter: 0001 Edge: 0002 train_loss= 262.91998 val_roc= 0.98916 val_auprc= 0.98738 val_apk= 1.00000 time= 0.76527\n",
      "Epoch: 0009 Iter: 0151 Edge: 0001 train_loss= 233.10559 val_roc= 0.95156 val_auprc= 0.94707 val_apk= 0.96197 time= 0.27710\n",
      "Epoch: 0009 Iter: 0301 Edge: 0002 train_loss= 275.06787 val_roc= 0.98934 val_auprc= 0.98764 val_apk= 1.00000 time= 0.89511\n",
      "Epoch: 0009 Iter: 0451 Edge: 0001 train_loss= 228.31003 val_roc= 0.95278 val_auprc= 0.94764 val_apk= 0.96292 time= 0.27839\n",
      "Epoch: 0009 Iter: 0601 Edge: 0002 train_loss= 271.52563 val_roc= 0.99033 val_auprc= 0.98948 val_apk= 1.00000 time= 0.77590\n",
      "Epoch: 0009 Iter: 0751 Edge: 0001 train_loss= 213.58395 val_roc= 0.95424 val_auprc= 0.94924 val_apk= 0.96470 time= 0.26688\n",
      "Epoch: 0009 Iter: 0901 Edge: 0002 train_loss= 270.30576 val_roc= 0.99065 val_auprc= 0.98955 val_apk= 1.00000 time= 0.75307\n",
      "Epoch: 0010 Iter: 0001 Edge: 0002 train_loss= 262.57523 val_roc= 0.99021 val_auprc= 0.98906 val_apk= 1.00000 time= 0.80879\n",
      "Epoch: 0010 Iter: 0151 Edge: 0001 train_loss= 205.42035 val_roc= 0.94704 val_auprc= 0.94216 val_apk= 0.96097 time= 0.28302\n",
      "Epoch: 0010 Iter: 0301 Edge: 0002 train_loss= 244.89941 val_roc= 0.99004 val_auprc= 0.98840 val_apk= 1.00000 time= 0.79937\n",
      "Epoch: 0010 Iter: 0451 Edge: 0001 train_loss= 220.23590 val_roc= 0.94428 val_auprc= 0.93916 val_apk= 0.96097 time= 0.28743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 Iter: 0601 Edge: 0002 train_loss= 278.74661 val_roc= 0.99065 val_auprc= 0.98967 val_apk= 1.00000 time= 0.82825\n",
      "Epoch: 0010 Iter: 0751 Edge: 0001 train_loss= 205.41565 val_roc= 0.95374 val_auprc= 0.94856 val_apk= 0.96992 time= 0.29056\n",
      "Epoch: 0010 Iter: 0901 Edge: 0002 train_loss= 236.31415 val_roc= 0.99081 val_auprc= 0.99001 val_apk= 1.00000 time= 0.81561\n",
      "Optimization finished!\n",
      "Total time: 1142.68585014\n",
      "Virtual memory: 7078535168\n",
      "RSS Memory: 583327744\n"
     ]
    }
   ],
   "source": [
    "print(\"Train model\")\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    minibatch.shuffle()\n",
    "    itr = 0\n",
    "    while not minibatch.end():\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = minibatch.next_minibatch_feed_dict(placeholders=placeholders)\n",
    "        feed_dict = minibatch.update_feed_dict(\n",
    "            feed_dict=feed_dict,\n",
    "            dropout=FLAGS.dropout,\n",
    "            placeholders=placeholders)\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        # Training step: run single weight update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.batch_edge_type_idx], feed_dict=feed_dict)\n",
    "        train_cost = outs[1]\n",
    "        batch_edge_type = outs[2]\n",
    "\n",
    "        if itr % PRINT_PROGRESS_EVERY == 0:\n",
    "            val_auc, val_auprc, val_apk = get_accuracy_scores(\n",
    "                minibatch.val_edges, minibatch.val_edges_false,\n",
    "                minibatch.idx2edge_type[minibatch.current_edge_type_idx])\n",
    "\n",
    "            print(\"Epoch:\", \"%04d\" % (epoch + 1), \"Iter:\", \"%04d\" % (itr + 1), \"Edge:\", \"%04d\" % batch_edge_type,\n",
    "                  \"train_loss=\", \"{:.5f}\".format(train_cost),\n",
    "                  \"val_roc=\", \"{:.5f}\".format(val_auc), \"val_auprc=\", \"{:.5f}\".format(val_auprc),\n",
    "                  \"val_apk=\", \"{:.5f}\".format(val_apk), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "        itr += 1\n",
    "print(\"Optimization finished!\")\n",
    "finish = time.time()\n",
    "memUse = ps.memory_info()\n",
    "print(\"Total time:\",finish-start)\n",
    "print('Virtual memory:', memUse.vms)\n",
    "print('RSS Memory:', memUse.rss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
