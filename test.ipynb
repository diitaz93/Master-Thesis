{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECAGON Training\n",
    "Test notebook for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from operator import itemgetter\n",
    "from itertools import combinations, chain\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pickle\n",
    "from decagon.deep.optimizer import DecagonOptimizer\n",
    "from decagon.deep.model import DecagonModel\n",
    "from decagon.deep.minibatch import EdgeMinibatchIterator\n",
    "from decagon.utility import rank_metrics, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on GPU\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psutil & time BEGIN\n",
    "start = time.time() #in seconds\n",
    "pid = os.getpid()\n",
    "ps= psutil.Process(pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data from previous computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to input file. Goes as parameter in script\n",
    "in_file = './data/data_structures/DECAGON/DECAGON_toy_genes_500_drugs_400_se_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = in_file.split('_')\n",
    "DSE = False\n",
    "BDM = False\n",
    "DOCK = False\n",
    "BIND = False\n",
    "if 'DSE' in words: DSE = True\n",
    "if 'BDM' in words: BDM = True\n",
    "if 'docking' in words: DOCK = True\n",
    "elif 'binding' in words: BIND = True\n",
    "d_text = DOCK*'_docking'+BIND*'_binding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge2name Imported successfully\n",
      "se_mono_name2idx Imported successfully\n",
      "gene2idx Imported successfully\n",
      "nonzero_feat Imported successfully\n",
      "edge_type2dim Imported successfully\n",
      "adj_mats_orig Imported successfully\n",
      "edge_type2decoder Imported successfully\n",
      "se_combo_name2idx Imported successfully\n",
      "drug2idx Imported successfully\n",
      "degrees Imported successfully\n",
      "edge_types Imported successfully\n",
      "num_edge_types Imported successfully\n",
      "num_feat Imported successfully\n",
      "feat Imported successfully\n"
     ]
    }
   ],
   "source": [
    "with open(in_file, 'rb') as f:\n",
    "    DS = pickle.load(f)\n",
    "    for key in DS.keys():\n",
    "        globals()[key]=DS[key]\n",
    "        print(key,\"Imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 400 4 600 False\n"
     ]
    }
   ],
   "source": [
    "n_genes = len(gene2idx)\n",
    "n_drugs = len(drug2idx)\n",
    "n_se_combo = len(se_combo_name2idx)\n",
    "n_se_mono = len(se_mono_name2idx)\n",
    "print(n_genes,n_drugs,n_se_combo,n_se_mono,DSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "        return 1. / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_scores(edges_pos, edges_neg, edge_type, noise=False):\n",
    "    \"\"\" Returns the AUROC, AUPRC and Accuracy of the dataset corresponding to the edge\n",
    "    'edge_type' given as a tuple. The parameters 'edges_pos' and 'edges_neg' are the list \n",
    "    of edges of positive and negative interactions respectively of a given dataset, i.e., \n",
    "    train, test or validation.\n",
    "    \"\"\"\n",
    "    feed_dict.update({placeholders['dropout']: 0})\n",
    "    feed_dict.update({placeholders['batch_edge_type_idx']: minibatch.edge_type2idx[edge_type]})\n",
    "    feed_dict.update({placeholders['batch_row_edge_type']: edge_type[0]})\n",
    "    feed_dict.update({placeholders['batch_col_edge_type']: edge_type[1]})\n",
    "    rec = sess.run(opt.predictions, feed_dict=feed_dict)\n",
    "    # Predict on set of edges\n",
    "    preds = []\n",
    "    for u, v in edges_pos:\n",
    "        score = sigmoid(rec[u, v])\n",
    "        preds.append(score)\n",
    "        if not noise:\n",
    "            assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] > 0, 'Problem 1'\n",
    "    preds_neg = []\n",
    "    for u, v in edges_neg:\n",
    "        score = sigmoid(rec[u, v])\n",
    "        preds_neg.append(score)\n",
    "        if not noise:\n",
    "            assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 0, 'Problem 0'\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    preds_all = np.nan_to_num(preds_all)\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "\n",
    "    roc_sc = metrics.roc_auc_score(labels_all, preds_all)\n",
    "    aupr_sc = metrics.average_precision_score(labels_all, preds_all)\n",
    "    acc = metrics.accuracy_score(labels_all, np.round(preds_all))\n",
    "\n",
    "    return roc_sc, aupr_sc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_placeholders(edge_types):\n",
    "    placeholders = {\n",
    "        'batch': tf.placeholder(tf.int32, name='batch'),\n",
    "        'batch_edge_type_idx': tf.placeholder(tf.int32, shape=(), name='batch_edge_type_idx'),\n",
    "        'batch_row_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_row_edge_type'),\n",
    "        'batch_col_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_col_edge_type'),\n",
    "        'degrees': tf.placeholder(tf.int32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    }\n",
    "    placeholders.update({\n",
    "        'adj_mats_%d,%d,%d' % (i, j, k): tf.sparse_placeholder(tf.float32)\n",
    "        for i, j in edge_types for k in range(edge_types[i,j])})\n",
    "    placeholders.update({\n",
    "        'feat_%d' % i: tf.sparse_placeholder(tf.float32)\n",
    "        for i, _ in edge_types})\n",
    "    return placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.05\n",
    "val_test_size = 0.15\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('neg_sample_size', 1, 'Negative sample size.')\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 10, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 64, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\n",
    "flags.DEFINE_float('weight_decay', 0, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_float('dropout', 0.1, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('max_margin', 0.1, 'Max margin parameter in hinge loss')\n",
    "flags.DEFINE_integer('batch_size', 128, 'minibatch size.')\n",
    "flags.DEFINE_boolean('bias', True, 'Bias term.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining placeholders\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining placeholders\")\n",
    "placeholders = construct_placeholders(edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MACHETAZO!! Soluciona el bug de Jupyter con tensorflow que proporciona un flag -f\n",
    "flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/data_structures/MINIBATCH/MINIBATCH_toy_genes_500_drugs_400_se_4_batchsize_128_valsize_0.15_noise_0.05\n"
     ]
    }
   ],
   "source": [
    "noise_str = bool(noise)*('_noise_' + str(noise))\n",
    "mb_file = 'data/data_structures/MINIBATCH/MINIBATCH_'+words[2]+d_text+\\\n",
    "            '_genes_'+str(n_genes)+'_drugs_'+\\\n",
    "            str(n_drugs)+'_se_'+str(n_se_combo)+'_batchsize_'+str(FLAGS.batch_size)+\\\n",
    "            '_valsize_'+str(val_test_size) + noise_str\n",
    "print(mb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(mb_file, 'rb') as f:\n",
    "    minibatch = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0, 0): 2,\n",
       " (0, 1, 0): 0,\n",
       " (1, 0, 0): 1,\n",
       " (1, 1, 0): 3,\n",
       " (1, 1, 1): 4,\n",
       " (1, 1, 2): 5,\n",
       " (1, 1, 3): 6}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch.edge_type2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create model\n",
      "WARNING:tensorflow:From decagon/deep/layers.py:93: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "print(\"Create model\")\n",
    "model = DecagonModel(\n",
    "    placeholders=placeholders,\n",
    "    num_feat=num_feat,\n",
    "    nonzero_feat=nonzero_feat,\n",
    "    edge_types=edge_types,\n",
    "    decoders=edge_type2decoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'decagonmodel/AddN_2:0' shape=(?, 32) dtype=float32>,\n",
       " <tf.Tensor 'decagonmodel/AddN_3:0' shape=(?, 32) dtype=float32>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent_inters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent_varies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create optimizer\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "print(\"Create optimizer\")\n",
    "with tf.name_scope('optimizer'):\n",
    "    opt = DecagonOptimizer(\n",
    "        embeddings=model.embeddings,\n",
    "        latent_inters=model.latent_inters,\n",
    "        latent_varies=model.latent_varies,\n",
    "        degrees=degrees,\n",
    "        edge_types=edge_types,\n",
    "        edge_type2dim=edge_type2dim,\n",
    "        placeholders=placeholders,\n",
    "        batch_size=FLAGS.batch_size,\n",
    "        margin=FLAGS.max_margin\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize session\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialize session\")\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "feed_dict = {}\n",
    "pre_train_time = time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = minibatch.next_minibatch_feed_dict(placeholders=placeholders)\n",
    "feed_dict = minibatch.update_feed_dict(\n",
    "            feed_dict=feed_dict,\n",
    "            dropout=FLAGS.dropout,\n",
    "            placeholders=placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 83, 324, 470, 482, 106, 375,  43, 202, 295,  66, 134,  55,   8,\n",
       "       239, 250, 373, 332, 179, 462, 387, 287, 146, 469, 444, 434,  89,\n",
       "       471, 135,  56, 345, 315, 466, 319, 387, 257, 426, 415, 419, 368,\n",
       "       227,  63, 320,  73, 254, 360, 257, 234,  80, 273,  53, 397, 167,\n",
       "        46,  43, 173,  28, 347,  94, 402, 385,  98, 320, 357, 445, 287,\n",
       "       434, 110, 393, 234, 339, 415, 233, 116, 373, 203, 254,  86, 103,\n",
       "       329, 302, 203, 425, 402, 114, 456,   2, 162, 190, 409, 472, 426,\n",
       "       356, 234,  56, 176, 237, 395, 245, 373, 244, 346, 405, 349, 130,\n",
       "       147, 469, 459, 208, 360, 170, 386, 425, 266,  24, 203,  83, 185,\n",
       "       251, 232,  78, 214, 479, 140, 446,  56, 356, 405, 382])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(opt.neg_samples,feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   1,   4,   6,  15,  17,  19,  21,  22,  28,  29,  30,\n",
       "        36,  38,  43,  46,  46,  51,  53,  55,  59,  59,  60,  60,  62,\n",
       "        64,  64,  66,  68,  70,  71,  73,  76,  79,  82,  83,  87,  88,\n",
       "        89,  90,  91,  93,  96,  98, 100, 100, 100, 108, 109, 110, 111,\n",
       "       113, 113, 114, 114, 115, 116, 118, 120, 122, 125, 126, 126, 130,\n",
       "       134, 138, 140, 141, 142, 142, 144, 146, 149, 151, 152, 154, 155,\n",
       "       162, 163, 168, 169, 170, 170, 171, 173, 173, 176, 177, 177, 178,\n",
       "       184, 185, 185, 188, 188, 191, 193, 195, 195, 197, 198, 199, 201,\n",
       "       204, 205, 211, 212, 214, 215, 221, 225, 225, 229, 232, 233, 239,\n",
       "       241, 245, 248, 252, 253, 253, 254, 254, 258, 258, 263], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(opt.inputs,feed_dict=feed_dict)[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = 'results_training/TRAIN_'+words[2]+d_text+DSE*('_DSE_'+str(n_se_mono))+BDM*('_BDM')\\\n",
    "            +'_genes_'+str(n_genes)+'_drugs_'+str(n_drugs)+'_se_'+str(n_se_combo)+'_epochs_'+\\\n",
    "            str(FLAGS.epochs)+'_h1_'+str(FLAGS.hidden1)+'_h2_'+str(FLAGS.hidden2)+\\\n",
    "            '_lr_'+str(FLAGS.learning_rate)+'_dropout_'+str(FLAGS.dropout)+'_valsize_'+\\\n",
    "            str(val_test_size) + noise_str\n",
    "#out_file = 'results_training/sandboxish'\n",
    "print(out_file)\n",
    "output_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Metric structures initialization\n",
    "val_metrics = np.zeros([FLAGS.epochs,num_edge_types,3])\n",
    "train_metrics = np.zeros([FLAGS.epochs,num_edge_types,3])\n",
    "# Start training\n",
    "print(\"Train model\")\n",
    "for epoch in range(FLAGS.epochs):\n",
    "    t = time.time()\n",
    "    minibatch.shuffle()\n",
    "    itr = 0\n",
    "    while not minibatch.end():\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = minibatch.next_minibatch_feed_dict(placeholders=placeholders)\n",
    "        feed_dict = minibatch.update_feed_dict(\n",
    "            feed_dict=feed_dict,\n",
    "            dropout=FLAGS.dropout,\n",
    "            placeholders=placeholders)\n",
    "        # Training step: run single weight update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.batch_edge_type_idx], feed_dict=feed_dict)\n",
    "        if (itr+1)%1000==0:print('Iteration',itr)\n",
    "        itr += 1\n",
    "    # Train & validation accuracy over all train data per epoch\n",
    "    print('======================================================================================================================')\n",
    "    print(\"Epoch\", \"%04d\" % (epoch + 1),'finished!')\n",
    "    print(\"Time=\", \"{:.5f}\".format(time.time()-t))\n",
    "    for r in range(num_edge_types):\n",
    "        i,j,k = minibatch.idx2edge_type[r]\n",
    "        print('Metrics for ', edge2name[i,j][k])\n",
    "        train_metrics[epoch,r,:] = get_accuracy_scores(\n",
    "            minibatch.train_edges[i,j][k], minibatch.train_edges_false[i,j][k],(i,j,k))\n",
    "        val_metrics[epoch,r,:] = get_accuracy_scores(\n",
    "            minibatch.val_edges[i,j][k], minibatch.val_edges_false[i,j][k],(i,j,k))\n",
    "        print(\"AUROC:Train=\", \"{:.4f}\".format(train_metrics[epoch,r,0])\n",
    "              ,\"Validation=\", \"{:.4f}\".format(val_metrics[epoch,r,0])\n",
    "              ,\"AUPRC:Train=\", \"{:.4f}\".format(train_metrics[epoch,r,1])\n",
    "              ,\"Validation=\", \"{:.4f}\".format(val_metrics[epoch,r,1])\n",
    "              ,\"Accuracy:Train=\", \"{:.4f}\".format(train_metrics[epoch,r,2])\n",
    "              ,\"Validation=\", \"{:.4f}\".format(val_metrics[epoch,r,2]))\n",
    "    output_data['val_metrics'] = val_metrics\n",
    "    output_data['train_metrics'] = train_metrics\n",
    "    output_data['epoch'] = epoch + 1\n",
    "    with open(out_file,'wb') as f:\n",
    "        pickle.dump(output_data, f, protocol=2)\n",
    "    \n",
    "# End of training. Metric structure handling   \n",
    "print(\"Optimization finished!\")\n",
    "test_metrics = np.zeros([num_edge_types,3])\n",
    "for et in range(num_edge_types):\n",
    "    i,j,k = minibatch.idx2edge_type[et]\n",
    "    test_metrics[et,:] = get_accuracy_scores(\n",
    "        minibatch.test_edges[i,j][k], minibatch.test_edges_false[i,j][k], (i,j,k),\n",
    "        noise=bool(noise))\n",
    "    print(\"Edge type=\", edge2name[i,j][k])\n",
    "    print(\"Edge type:\", \"%04d\" % et, \"Test AUROC score\", \"{:.5f}\".format(test_metrics[et,0]))\n",
    "    print(\"Edge type:\", \"%04d\" % et, \"Test AUPRC score\", \"{:.5f}\".format(test_metrics[et,1]))\n",
    "    print(\"Edge type:\", \"%04d\" % et, \"Test Accuracy score\", \"{:.5f}\".format(test_metrics[et,2]))\n",
    "    print()\n",
    "output_data['test_metrics'] = test_metrics\n",
    "memUse = ps.memory_info()\n",
    "print('Virtual memory:', memUse.vms*1e-09,'Gb')\n",
    "print('RSS Memory:', memUse.rss*1e-09,'Gb')\n",
    "train_time=time.time()-pre_train_time\n",
    "output_data['pre_train_time'] = pre_train_time\n",
    "output_data['train_time'] = train_time\n",
    "output_data['edge2name'] = edge2name\n",
    "output_data['drug2idx'] = drug2idx\n",
    "output_data['gene2idx'] = gene2idx\n",
    "output_data['vms'] = memUse.vms\n",
    "output_data['rss'] = memUse.rss\n",
    "with open(out_file,'wb') as f:\n",
    "    pickle.dump(output_data, f, protocol=2)\n",
    "print('Total time:', datetime.timedelta(seconds=time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
