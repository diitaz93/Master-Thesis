{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox\n",
    "Notebook to test code before real implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ ONLY PYTHON 2 #########################\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from operator import itemgetter\n",
    "from itertools import combinations, chain, product\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pickle\n",
    "from decagon.deep.optimizer import DecagonOptimizer\n",
    "from decagon.deep.model import DecagonModel\n",
    "from decagon.deep.minibatch import EdgeMinibatchIterator\n",
    "from decagon.utility import rank_metrics, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import pickle\n",
    "from pybdm import BDM\n",
    "from pybdm.utils import decompose_dataset\n",
    "from pybdm.partitions import PartitionIgnore\n",
    "from pybdm.partitions import PartitionRecursive\n",
    "from joblib import Parallel, delayed\n",
    "from joblib import parallel_backend\n",
    "from data.algorithms import PerturbationExperiment, NodePerturbationExperiment\n",
    "import math\n",
    "import datetime as dt\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = 'data/data_structures/BDM/EDGES_PPI_real_genes_19081'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(in_file,'rb') as f:\n",
    "    edge_complexity = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'data/data_structures/DS/DS_real_DSE_9700_genes_16837_drugs_636_se_7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene2idx Imported successfully\n",
      "drug2idx Imported successfully\n",
      "se_mono_name2idx Imported successfully\n",
      "se_combo_name2idx Imported successfully\n",
      "ddi_adj_list Imported successfully\n",
      "ddi_degrees_list Imported successfully\n",
      "dti_adj Imported successfully\n",
      "ppi_adj Imported successfully\n",
      "ppi_degrees Imported successfully\n",
      "drug_feat Imported successfully\n"
     ]
    }
   ],
   "source": [
    "with open(ds,'rb') as f:\n",
    "    DS = pickle.load(f)\n",
    "    for key in DS.keys():\n",
    "        globals()[key]=DS[key]\n",
    "        print(key,\"Imported successfully\")\n",
    "old_genes = len(gene2idx)\n",
    "old_drugs = len(drug2idx)\n",
    "old_se_combo = len(se_combo_name2idx)\n",
    "old_se_mono = len(se_mono_name2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364084561,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(edge_complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_mat = edge_complexity.reshape([19081,19081])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ppi_mat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f43dae0999ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m \u001b[0;31m# The addition of this value makes the number of nonzero to coincide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Elementwise multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrue_cmplx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mppi_mat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomplexity_mat\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# Take abs and sort from largest to smallest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcmplx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_cmplx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrue_cmplx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ppi_mat' is not defined"
     ]
    }
   ],
   "source": [
    "# OPTION 1: USE ELEMENTWISE MULT TO FORM DENSE MATRIX \n",
    "eps = 0.0001 # The addition of this value makes the number of nonzero to coincide\n",
    "# Elementwise multiplication\n",
    "true_cmplx = np.multiply(ppi_mat,complexity_mat+eps)\n",
    "# Take abs and sort from largest to smallest\n",
    "cmplx = np.squeeze(np.asarray(np.abs(true_cmplx[true_cmplx != 0])))\n",
    "sorted_cmplx = np.sort(cmplx)[::-1]\n",
    "# Get the cutting treshold based on the cutting fraction of data\n",
    "l = len(sorted_cmplx)\n",
    "threshold = sorted_cmplx[np.floor(l*(1-cut_frac)).astype(int)]\n",
    "# Choose the entries that exceed the threshold, discard the rest\n",
    "new_ppi_adj = (np.abs(true_cmplx)>threshold).astype(int)\n",
    "print('Nonzero entries before',np.count_nonzero(true_cmplx))\n",
    "print('Nonzero entries after',np.count_nonzero(new_ppi_adj))\n",
    "print('Is it symmetric?',np.array_equal(new_ppi_adj,new_ppi_adj.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring BDM to make it sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/data_structures/BDM/DDI_BDM_real_se_964_drugs_639_juadia72','rb') as f:\n",
    "    DS = pickle.load(f)\n",
    "    for key in DS.keys():\n",
    "        globals()[key]=DS[key]\n",
    "        print(key,\"Imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_nodes = nodebdm_ddi_list[0]\n",
    "srt_nodes = sort(sim_nodes)\n",
    "sim_add = add_edgebdm_ddi_list[0]\n",
    "srt_add = sort(sim_add)\n",
    "sim_rem = rem_edgebdm_ddi_list[0]\n",
    "srt_rem = sort(sim_rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical differentiation\n",
    "x = np.arange(len(sim_nodes))\n",
    "dy_nodes = np.zeros(srt_nodes.shape,np.float)\n",
    "dy_nodes[0:-1] = np.diff(srt_nodes)/np.diff(x)\n",
    "dy_nodes[-1] = (srt_nodes[-1] - srt_nodes[-2])/(x[-1] - x[-2])\n",
    "dy_nodes = dy_nodes/np.linalg.norm(dy_nodes)\n",
    "dy_add = np.zeros(srt_add.shape,np.float)\n",
    "dy_add[0:-1] = np.diff(srt_nodes)/np.diff(x)\n",
    "dy_add[-1] = (srt_add[-1] - srt_add[-2])/(x[-1] - x[-2])\n",
    "dy_add = dy_add/np.linalg.norm(dy_add)\n",
    "dy_rem = np.zeros(srt_rem.shape,np.float)\n",
    "dy_rem[0:-1] = np.diff(srt_rem)/np.diff(x)\n",
    "dy_rem[-1] = (srt_rem[-1] - srt_rem[-2])/(x[-1] - x[-2])\n",
    "dy_rem = dy_rem/np.linalg.norm(dy_rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete convolution of derivative\n",
    "win = 50\n",
    "v = (1/win)*np.ones(win)\n",
    "conv_dy_nodes = np.convolve(dy_nodes,v,mode='same')\n",
    "conv_dy_add = np.convolve(dy_add,v,mode='same')\n",
    "conv_dy_rem = np.convolve(dy_rem,v,mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=[15,12])\n",
    "subplot(3,3,1)\n",
    "plot(sim_nodes)\n",
    "title('Node BDM')\n",
    "subplot(3,3,2)\n",
    "hist(sim_nodes)\n",
    "title('Node BDM')\n",
    "subplot(3,3,3)\n",
    "#plot(srt_nodes)\n",
    "plot(dy_nodes)\n",
    "plot(conv_dy_nodes)\n",
    "title('Node BDM')\n",
    "subplot(3,3,4)\n",
    "plot(sim_add)\n",
    "title('Add edges')\n",
    "subplot(3,3,5)\n",
    "hist(sim_add)\n",
    "title('Add edges')\n",
    "subplot(3,3,6)\n",
    "#plot(srt_add)\n",
    "plot(dy_add)\n",
    "plot(conv_dy_add)\n",
    "title('Node BDM')\n",
    "subplot(3,3,7)\n",
    "plot(sim_rem)\n",
    "title('Remove edges')\n",
    "subplot(3,3,8)\n",
    "hist(sim_rem)\n",
    "title('Remove edges')\n",
    "subplot(3,3,9)\n",
    "#plot(srt_rem)\n",
    "plot(dy_rem)\n",
    "plot(conv_dy_rem)\n",
    "title('Node BDM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Biological meaning of adding edges\n",
    "2. How to calculate thresholds\n",
    "    Filtering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_threshold = -500\n",
    "up_threshold = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = nodebdm_ppi<down_threshold\n",
    "pos = nodebdm_ppi>up_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_nodebdm_ppi = neg.astype(int)*-1+pos.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(bin_nodebdm_ppi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create random symmetric adj matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test random adj matrix\n",
    "s = 100\n",
    "b = np.random.randint(0,2,size=[s,s])\n",
    "sym = np.floor((b+b.T)/2).astype(int)\n",
    "sym[:,3] = np.zeros(s)\n",
    "sym[3,:] = np.zeros(s)\n",
    "ppi_adj = sp.csc_matrix(sym)\n",
    "print(np.count_nonzero(sym))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisar si adj mats contienen el 1 en la diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/data_structures/DS/DS_real_DSE_9700_genes_16837_drugs_636_se_7','rb') as f:\n",
    "    DS = pickle.load(f)\n",
    "    for key in DS.keys():\n",
    "        globals()[key]=DS[key]\n",
    "        print(key,\"Imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi_adj_list[0].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWOSIDES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.read_csv('../Thesis_datasets/TWOSIDES/small.csv',sep=',',usecols=[0,1,2,3,4,5])\n",
    "DEC = pd.read_csv('data/original_data/bio-decagon-combo.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Dictionaries Side effects\n",
    "`meddra.tsv` is a database that contains names of side effects in both wanted ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_se = pd.read_csv('../Thesis_datasets/SIDER/meddra.tsv', sep = '\\t'\\\n",
    "                     ,header=None).rename(columns={0:'UMLS',1:'kind',2:'MedDRA',3:'name'})\n",
    "cui = pd.unique(map_se['UMLS'].values)\n",
    "meddra = pd.unique(map_se['MedDRA'].values)\n",
    "ses = pd.unique(map_se['name'].values)\n",
    "print('Total',len(map_se))\n",
    "print('CUI',len(cui))\n",
    "print('MedDRA',len(meddra))\n",
    "print('Side Effects',len(ses))\n",
    "map_se.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that translates UMLS CUIs to MedDRA IDs\n",
    "umls2meddra = defaultdict(set)\n",
    "for se in map_se.index:\n",
    "    umls2meddra[map_se.loc[se,'UMLS']] = map_se.loc[se,'MedDRA']\n",
    "# Dictionary that translates MedDRA IDs to UMLS CUIs\n",
    "meddra2umls = defaultdict(set)\n",
    "for se in map_se.index:\n",
    "    meddra2umls[map_se.loc[se,'MedDRA']] = map_se.loc[se,'UMLS']\n",
    "# Dictionary that translates UMLS CUIs to name\n",
    "names = map_se[map_se['kind'].str.match('PT',na=False)].reset_index(drop=True)\n",
    "print(len(names.index))\n",
    "UMLS2names = {}\n",
    "for se in names.index:\n",
    "    UMLS2names[names.loc[se,'UMLS']] = names.loc[se,'name']\n",
    "# Verify numbers\n",
    "print(len(umls2meddra))\n",
    "print(len(meddra2umls))\n",
    "print(len(UMLS2names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that translates UMLS CUIs to name\n",
    "names = map_se[map_se['kind'].str.match('PT',na=False)].reset_index(drop=True)\n",
    "print(len(names.index))\n",
    "UMLS2names = {}\n",
    "for se in names.index:\n",
    "    UMLS2names[names.loc[se,'UMLS']] = names.loc[se,'name']\n",
    "print(len(UMLS2names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "for i in DF['condition_meddra_id']:\n",
    "    ids.append(meddra2umls[i])\n",
    "DF['Condition'] = ids\n",
    "DF = DF.drop(columns=['condition_meddra_id'])\n",
    "name_list = []\n",
    "for i in DF['Condition']:\n",
    "    name_list.append(UMLS2names[i])\n",
    "DF['Condition_name'] = name_list\n",
    "DF = DF.drop(columns=['condition_concept_name'])\n",
    "DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_drug = pd.read_csv('../Thesis_datasets/SIDER/drug_names.tsv',sep = '\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_drug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing fixed unigram candidate sampler found in optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "a = [7,0,20,8,33,9]\n",
    "labels = tf.reshape(tf.constant(a,dtype=tf.int64),[6,1])\n",
    "sampled_ids, true_expected_count, sampled_expected_count = tf.nn.fixed_unigram_candidate_sampler(\n",
    "   true_classes = labels,\n",
    "   num_true = 1,\n",
    "   num_sampled = 20,\n",
    "   unique = False,\n",
    "   range_max = np.shape(a)[0],\n",
    "   unigrams = [ 10, 10, 10, 10, 50, 10 ]\n",
    ")\n",
    "sample = tf.gather( labels, sampled_ids )\n",
    "print(sess.run( true_expected_count ))\n",
    "print(sess.run( sampled_ids ))\n",
    "print(sess.run( sampled_expected_count ))\n",
    "print(sess.run( sample ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = 'data/data_structures/DECAGON/DECAGON_real_affinities_genes_16814_drugs_276_se_7'\n",
    "with open(in_file, 'rb') as f:\n",
    "    DS = pickle.load(f)\n",
    "    for key in DS.keys():\n",
    "        globals()[key]=DS[key]\n",
    "        print(key,\"Imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mats_orig[1,0][0].todense().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('../Thesis_datasets/DrugBank/drugbank_all_full_database.xml/full database.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
