{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECAGON Training\n",
    "Test notebook for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from operator import itemgetter\n",
    "from itertools import combinations, chain\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pickle\n",
    "from decagon.deep.optimizer import DecagonOptimizer\n",
    "from decagon.deep.model import DecagonModel\n",
    "from decagon.deep.minibatch import EdgeMinibatchIterator\n",
    "from decagon.utility import rank_metrics, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on GPU\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psutil & time BEGIN\n",
    "start = time.time() #in seconds\n",
    "pid = os.getpid()\n",
    "ps= psutil.Process(pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data from previous computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to input file. Goes as parameter in script\n",
    "in_file = './data/data_structures/DECAGON/DECAGON_toy_genes_500_drugs_400_se_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = in_file.split('_')\n",
    "DSE = False\n",
    "BDM = False\n",
    "DOCK = False\n",
    "BIND = False\n",
    "if 'DSE' in words: DSE = True\n",
    "if 'BDM' in words: BDM = True\n",
    "if 'docking' in words: DOCK = True\n",
    "elif 'binding' in words: BIND = True\n",
    "d_text = DOCK*'_docking'+BIND*'_binding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge2name Imported successfully\n",
      "se_mono_name2idx Imported successfully\n",
      "gene2idx Imported successfully\n",
      "nonzero_feat Imported successfully\n",
      "edge_type2dim Imported successfully\n",
      "adj_mats_orig Imported successfully\n",
      "edge_type2decoder Imported successfully\n",
      "se_combo_name2idx Imported successfully\n",
      "drug2idx Imported successfully\n",
      "degrees Imported successfully\n",
      "edge_types Imported successfully\n",
      "num_edge_types Imported successfully\n",
      "num_feat Imported successfully\n",
      "feat Imported successfully\n"
     ]
    }
   ],
   "source": [
    "with open(in_file, 'rb') as f:\n",
    "    DS = pickle.load(f)\n",
    "    for key in DS.keys():\n",
    "        globals()[key]=DS[key]\n",
    "        print(key,\"Imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 1, (0, 1): 1, (1, 0): 1, (1, 1): 4}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 400 4 600 False\n"
     ]
    }
   ],
   "source": [
    "n_genes = len(gene2idx)\n",
    "n_drugs = len(drug2idx)\n",
    "n_se_combo = len(se_combo_name2idx)\n",
    "n_se_mono = len(se_mono_name2idx)\n",
    "print(n_genes,n_drugs,n_se_combo,n_se_mono,DSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "        return 1. / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_scores(edges_pos, edges_neg, edge_type, noise=False):\n",
    "    \"\"\" Returns the AUROC, AUPRC and Accuracy of the dataset corresponding to the edge\n",
    "    'edge_type' given as a tuple. The parameters 'edges_pos' and 'edges_neg' are the list \n",
    "    of edges of positive and negative interactions respectively of a given dataset, i.e., \n",
    "    train, test or validation.\n",
    "    \"\"\"\n",
    "    feed_dict.update({placeholders['dropout']: 0})\n",
    "    feed_dict.update({placeholders['batch_edge_type_idx']: minibatch.edge_type2idx[edge_type]})\n",
    "    feed_dict.update({placeholders['batch_row_edge_type']: edge_type[0]})\n",
    "    feed_dict.update({placeholders['batch_col_edge_type']: edge_type[1]})\n",
    "    rec = sess.run(opt.predictions, feed_dict=feed_dict)\n",
    "    # Predict on set of edges\n",
    "    preds = []\n",
    "    for u, v in edges_pos:\n",
    "        score = sigmoid(rec[u, v])\n",
    "        preds.append(score)\n",
    "        if not noise:\n",
    "            assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] > 0, 'Problem 1'\n",
    "    preds_neg = []\n",
    "    for u, v in edges_neg:\n",
    "        score = sigmoid(rec[u, v])\n",
    "        preds_neg.append(score)\n",
    "        if not noise:\n",
    "            assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 0, 'Problem 0'\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    preds_all = np.nan_to_num(preds_all)\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "\n",
    "    roc_sc = metrics.roc_auc_score(labels_all, preds_all)\n",
    "    aupr_sc = metrics.average_precision_score(labels_all, preds_all)\n",
    "    acc = metrics.accuracy_score(labels_all, np.round(preds_all))\n",
    "\n",
    "    return roc_sc, aupr_sc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_placeholders(edge_types):\n",
    "    placeholders = {\n",
    "        'batch': tf.placeholder(tf.int32, name='batch'),\n",
    "        'batch_edge_type_idx': tf.placeholder(tf.int32, shape=(), name='batch_edge_type_idx'),\n",
    "        'batch_row_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_row_edge_type'),\n",
    "        'batch_col_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_col_edge_type'),\n",
    "        'degrees': tf.placeholder(tf.int32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    }\n",
    "    placeholders.update({\n",
    "        'adj_mats_%d,%d,%d' % (i, j, k): tf.sparse_placeholder(tf.float32)\n",
    "        for i, j in edge_types for k in range(edge_types[i,j])})\n",
    "    placeholders.update({\n",
    "        'feat_%d' % i: tf.sparse_placeholder(tf.float32)\n",
    "        for i, _ in edge_types})\n",
    "    return placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.05\n",
    "val_test_size = 0.15\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('neg_sample_size', 1, 'Negative sample size.')\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 10, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 64, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\n",
    "flags.DEFINE_float('weight_decay', 0, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_float('dropout', 0.1, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('max_margin', 0.1, 'Max margin parameter in hinge loss')\n",
    "flags.DEFINE_integer('batch_size', 128, 'minibatch size.')\n",
    "flags.DEFINE_boolean('bias', True, 'Bias term.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining placeholders\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining placeholders\")\n",
    "placeholders = construct_placeholders(edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MACHETAZO!! Soluciona el bug de Jupyter con tensorflow que proporciona un flag -f\n",
    "flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/data_structures/MINIBATCH/MINIBATCH_toy_genes_500_drugs_400_se_4_batchsize_128_valsize_0.15_noise_0.05\n"
     ]
    }
   ],
   "source": [
    "noise_str = bool(noise)*('_noise_' + str(noise))\n",
    "mb_file = 'data/data_structures/MINIBATCH/MINIBATCH_'+words[2]+d_text+\\\n",
    "            '_genes_'+str(n_genes)+'_drugs_'+\\\n",
    "            str(n_drugs)+'_se_'+str(n_se_combo)+'_batchsize_'+str(FLAGS.batch_size)+\\\n",
    "            '_valsize_'+str(val_test_size) + noise_str\n",
    "print(mb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(mb_file, 'rb') as f:\n",
    "    minibatch = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create model\n",
      "WARNING:tensorflow:From decagon/deep/layers.py:93: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "print(\"Create model\")\n",
    "model = DecagonModel(\n",
    "    placeholders=placeholders,\n",
    "    num_feat=num_feat,\n",
    "    nonzero_feat=nonzero_feat,\n",
    "    edge_types=edge_types,\n",
    "    decoders=edge_type2decoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create optimizer\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "print(\"Create optimizer\")\n",
    "with tf.name_scope('optimizer'):\n",
    "    opt = DecagonOptimizer(\n",
    "        embeddings=model.embeddings,\n",
    "        latent_inters=model.latent_inters,\n",
    "        latent_varies=model.latent_varies,\n",
    "        degrees=degrees,\n",
    "        edge_types=edge_types,\n",
    "        edge_type2dim=edge_type2dim,\n",
    "        placeholders=placeholders,\n",
    "        batch_size=FLAGS.batch_size,\n",
    "        margin=FLAGS.max_margin\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize session\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialize session\")\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "feed_dict = {}\n",
    "pre_train_time = time.time()-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results_training/TRAIN_toy_genes_500_drugs_400_se_4_epochs_10_h1_64_h2_32_lr_0.001_dropout_0.1_valsize_0.15_noise_0.05\n"
     ]
    }
   ],
   "source": [
    "out_file = 'results_training/TRAIN_'+words[2]+d_text+DSE*('_DSE_'+str(n_se_mono))+BDM*('_BDM')\\\n",
    "            +'_genes_'+str(n_genes)+'_drugs_'+str(n_drugs)+'_se_'+str(n_se_combo)+'_epochs_'+\\\n",
    "            str(FLAGS.epochs)+'_h1_'+str(FLAGS.hidden1)+'_h2_'+str(FLAGS.hidden2)+\\\n",
    "            '_lr_'+str(FLAGS.learning_rate)+'_dropout_'+str(FLAGS.dropout)+'_valsize_'+\\\n",
    "            str(val_test_size) + noise_str\n",
    "#out_file = 'results_training/sandboxish'\n",
    "print(out_file)\n",
    "output_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model\n",
      "======================================================================================================================\n",
      "Epoch 0001 finished!\n",
      "Time= 8.94718\n",
      "Metrics for  DTI\n",
      "AUROC:Train= 0.8414 Validation= 0.6525 AUPRC:Train= 0.8781 Validation= 0.6466 Accuracy:Train= 0.8182 Validation= 0.5849\n",
      "Metrics for  TDI\n",
      "AUROC:Train= 0.8141 Validation= 0.6226 AUPRC:Train= 0.8750 Validation= 0.6716 Accuracy:Train= 0.8083 Validation= 0.5943\n",
      "Metrics for  PPI\n",
      "AUROC:Train= 0.9715 Validation= 0.7584 AUPRC:Train= 0.9638 Validation= 0.7651 Accuracy:Train= 0.9188 Validation= 0.7065\n",
      "Metrics for  0\n",
      "AUROC:Train= 0.5014 Validation= 0.4979 AUPRC:Train= 0.5015 Validation= 0.4979 Accuracy:Train= 0.5009 Validation= 0.4984\n",
      "Metrics for  1\n",
      "AUROC:Train= 0.5079 Validation= 0.4829 AUPRC:Train= 0.5056 Validation= 0.4791 Accuracy:Train= 0.5036 Validation= 0.4927\n",
      "Metrics for  2\n",
      "AUROC:Train= 0.5202 Validation= 0.5161 AUPRC:Train= 0.5231 Validation= 0.5088 Accuracy:Train= 0.5115 Validation= 0.5056\n",
      "Metrics for  3\n",
      "AUROC:Train= 0.5616 Validation= 0.5017 AUPRC:Train= 0.5404 Validation= 0.5314 Accuracy:Train= 0.5674 Validation= 0.5175\n",
      "======================================================================================================================\n",
      "Epoch 0002 finished!\n",
      "Time= 6.75043\n",
      "Metrics for  DTI\n",
      "AUROC:Train= 0.9177 Validation= 0.7227 AUPRC:Train= 0.9301 Validation= 0.7162 Accuracy:Train= 0.8715 Validation= 0.6321\n",
      "Metrics for  TDI\n",
      "AUROC:Train= 0.9123 Validation= 0.6319 AUPRC:Train= 0.9380 Validation= 0.6747 Accuracy:Train= 0.8399 Validation= 0.5755\n",
      "Metrics for  PPI\n",
      "AUROC:Train= 0.9841 Validation= 0.7949 AUPRC:Train= 0.9831 Validation= 0.7964 Accuracy:Train= 0.9432 Validation= 0.7663\n",
      "Metrics for  0\n",
      "AUROC:Train= 0.5098 Validation= 0.5005 AUPRC:Train= 0.5085 Validation= 0.4968 Accuracy:Train= 0.5029 Validation= 0.4994\n",
      "Metrics for  1\n",
      "AUROC:Train= 0.5187 Validation= 0.4846 AUPRC:Train= 0.5168 Validation= 0.4891 Accuracy:Train= 0.5124 Validation= 0.4924\n",
      "Metrics for  2\n",
      "AUROC:Train= 0.5293 Validation= 0.5140 AUPRC:Train= 0.5284 Validation= 0.5210 Accuracy:Train= 0.5147 Validation= 0.5022\n",
      "Metrics for  3\n",
      "AUROC:Train= 0.5988 Validation= 0.5460 AUPRC:Train= 0.5889 Validation= 0.5061 Accuracy:Train= 0.5730 Validation= 0.5439\n",
      "======================================================================================================================\n",
      "Epoch 0003 finished!\n",
      "Time= 6.44917\n",
      "Metrics for  DTI\n",
      "AUROC:Train= 0.9380 Validation= 0.7693 AUPRC:Train= 0.9490 Validation= 0.7623 Accuracy:Train= 0.8696 Validation= 0.6415\n",
      "Metrics for  TDI\n",
      "AUROC:Train= 0.9294 Validation= 0.6785 AUPRC:Train= 0.9424 Validation= 0.6924 Accuracy:Train= 0.8399 Validation= 0.5755\n",
      "Metrics for  PPI\n",
      "AUROC:Train= 0.9907 Validation= 0.8228 AUPRC:Train= 0.9807 Validation= 0.8644 Accuracy:Train= 0.9687 Validation= 0.7772\n",
      "Metrics for  0\n",
      "AUROC:Train= 0.5123 Validation= 0.5017 AUPRC:Train= 0.5089 Validation= 0.5013 Accuracy:Train= 0.5064 Validation= 0.5032\n",
      "Metrics for  1\n",
      "AUROC:Train= 0.5245 Validation= 0.4923 AUPRC:Train= 0.5204 Validation= 0.4999 Accuracy:Train= 0.5158 Validation= 0.4940\n",
      "Metrics for  2\n",
      "AUROC:Train= 0.5486 Validation= 0.5126 AUPRC:Train= 0.5396 Validation= 0.5192 Accuracy:Train= 0.5250 Validation= 0.5011\n",
      "Metrics for  3\n",
      "AUROC:Train= 0.6080 Validation= 0.5005 AUPRC:Train= 0.6062 Validation= 0.4899 Accuracy:Train= 0.5674 Validation= 0.4912\n",
      "======================================================================================================================\n",
      "Epoch 0004 finished!\n",
      "Time= 6.39885\n",
      "Metrics for  DTI\n",
      "AUROC:Train= 0.9697 Validation= 0.7871 AUPRC:Train= 0.9720 Validation= 0.7683 Accuracy:Train= 0.8972 Validation= 0.5943\n",
      "Metrics for  TDI\n",
      "AUROC:Train= 0.9779 Validation= 0.7593 AUPRC:Train= 0.9827 Validation= 0.7977 Accuracy:Train= 0.9289 Validation= 0.6604\n",
      "Metrics for  PPI\n",
      "AUROC:Train= 0.9907 Validation= 0.8379 AUPRC:Train= 0.9841 Validation= 0.8871 Accuracy:Train= 0.9640 Validation= 0.8098\n",
      "Metrics for  0\n",
      "AUROC:Train= 0.5149 Validation= 0.5056 AUPRC:Train= 0.5101 Validation= 0.5028 Accuracy:Train= 0.5119 Validation= 0.5051\n",
      "Metrics for  1\n",
      "AUROC:Train= 0.5330 Validation= 0.5110 AUPRC:Train= 0.5249 Validation= 0.5085 Accuracy:Train= 0.5218 Validation= 0.5046\n",
      "Metrics for  2\n",
      "AUROC:Train= 0.5644 Validation= 0.5190 AUPRC:Train= 0.5510 Validation= 0.5276 Accuracy:Train= 0.5483 Validation= 0.4978\n",
      "Metrics for  3\n",
      "AUROC:Train= 0.6697 Validation= 0.5506 AUPRC:Train= 0.6708 Validation= 0.5489 Accuracy:Train= 0.5955 Validation= 0.5702\n",
      "======================================================================================================================\n",
      "Epoch 0005 finished!\n",
      "Time= 6.86110\n",
      "Metrics for  DTI\n",
      "AUROC:Train= 0.9766 Validation= 0.8085 AUPRC:Train= 0.9785 Validation= 0.8209 Accuracy:Train= 0.9190 Validation= 0.6509\n",
      "Metrics for  TDI\n",
      "AUROC:Train= 0.9868 Validation= 0.8163 AUPRC:Train= 0.9872 Validation= 0.8268 Accuracy:Train= 0.9308 Validation= 0.6415\n",
      "Metrics for  PPI\n",
      "AUROC:Train= 0.9931 Validation= 0.8352 AUPRC:Train= 0.9869 Validation= 0.8778 Accuracy:Train= 0.9710 Validation= 0.8207\n",
      "Metrics for  0\n",
      "AUROC:Train= 0.5174 Validation= 0.5058 AUPRC:Train= 0.5127 Validation= 0.5027 Accuracy:Train= 0.5140 Validation= 0.5074\n",
      "Metrics for  1\n",
      "AUROC:Train= 0.5336 Validation= 0.4973 AUPRC:Train= 0.5220 Validation= 0.4997 Accuracy:Train= 0.5232 Validation= 0.4990\n",
      "Metrics for  2\n",
      "AUROC:Train= 0.5832 Validation= 0.5268 AUPRC:Train= 0.5635 Validation= 0.5250 Accuracy:Train= 0.5661 Validation= 0.5034\n",
      "Metrics for  3\n",
      "AUROC:Train= 0.6838 Validation= 0.5248 AUPRC:Train= 0.6826 Validation= 0.5320 Accuracy:Train= 0.6011 Validation= 0.5000\n",
      "======================================================================================================================\n",
      "Epoch 0006 finished!\n",
      "Time= 6.51421\n",
      "Metrics for  DTI\n",
      "AUROC:Train= 0.9737 Validation= 0.7939 AUPRC:Train= 0.9721 Validation= 0.7898 Accuracy:Train= 0.9051 Validation= 0.6604\n",
      "Metrics for  TDI\n",
      "AUROC:Train= 0.9867 Validation= 0.7907 AUPRC:Train= 0.9848 Validation= 0.8091 Accuracy:Train= 0.9387 Validation= 0.6698\n",
      "Metrics for  PPI\n",
      "AUROC:Train= 0.9939 Validation= 0.8442 AUPRC:Train= 0.9841 Validation= 0.8907 Accuracy:Train= 0.9687 Validation= 0.8261\n",
      "Metrics for  0\n",
      "AUROC:Train= 0.5179 Validation= 0.5108 AUPRC:Train= 0.5138 Validation= 0.5088 Accuracy:Train= 0.5154 Validation= 0.5076\n",
      "Metrics for  1\n",
      "AUROC:Train= 0.5384 Validation= 0.5137 AUPRC:Train= 0.5247 Validation= 0.5086 Accuracy:Train= 0.5269 Validation= 0.5129\n",
      "Metrics for  2\n",
      "AUROC:Train= 0.6022 Validation= 0.5464 AUPRC:Train= 0.5773 Validation= 0.5382 Accuracy:Train= 0.5766 Validation= 0.5124\n",
      "Metrics for  3\n",
      "AUROC:Train= 0.7014 Validation= 0.4943 AUPRC:Train= 0.6923 Validation= 0.5113 Accuracy:Train= 0.6367 Validation= 0.4912\n",
      "======================================================================================================================\n",
      "Epoch 0007 finished!\n",
      "Time= 6.28288\n",
      "Metrics for  DTI\n",
      "AUROC:Train= 0.9825 Validation= 0.8120 AUPRC:Train= 0.9820 Validation= 0.8029 Accuracy:Train= 0.9308 Validation= 0.6509\n",
      "Metrics for  TDI\n",
      "AUROC:Train= 0.9904 Validation= 0.7967 AUPRC:Train= 0.9908 Validation= 0.7997 Accuracy:Train= 0.9289 Validation= 0.6415\n",
      "Metrics for  PPI\n",
      "AUROC:Train= 0.9929 Validation= 0.8478 AUPRC:Train= 0.9837 Validation= 0.8952 Accuracy:Train= 0.9803 Validation= 0.8424\n",
      "Metrics for  0\n",
      "AUROC:Train= 0.5209 Validation= 0.5103 AUPRC:Train= 0.5169 Validation= 0.5090 Accuracy:Train= 0.5159 Validation= 0.5063\n",
      "Metrics for  1\n",
      "AUROC:Train= 0.5444 Validation= 0.5230 AUPRC:Train= 0.5307 Validation= 0.5175 Accuracy:Train= 0.5332 Validation= 0.5281\n",
      "Metrics for  2\n",
      "AUROC:Train= 0.5979 Validation= 0.5495 AUPRC:Train= 0.5730 Validation= 0.5432 Accuracy:Train= 0.5704 Validation= 0.5404\n",
      "Metrics for  3\n",
      "AUROC:Train= 0.7276 Validation= 0.5035 AUPRC:Train= 0.7065 Validation= 0.5139 Accuracy:Train= 0.6629 Validation= 0.4912\n",
      "======================================================================================================================\n",
      "Epoch 0008 finished!\n",
      "Time= 6.58491\n",
      "Metrics for  DTI\n",
      "AUROC:Train= 0.9892 Validation= 0.8199 AUPRC:Train= 0.9887 Validation= 0.8116 Accuracy:Train= 0.9348 Validation= 0.6132\n",
      "Metrics for  TDI\n",
      "AUROC:Train= 0.9967 Validation= 0.8010 AUPRC:Train= 0.9964 Validation= 0.8294 Accuracy:Train= 0.9625 Validation= 0.6509\n",
      "Metrics for  PPI\n",
      "AUROC:Train= 0.9971 Validation= 0.8443 AUPRC:Train= 0.9967 Validation= 0.8963 Accuracy:Train= 0.9756 Validation= 0.8315\n",
      "Metrics for  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC:Train= 0.5239 Validation= 0.5063 AUPRC:Train= 0.5192 Validation= 0.5069 Accuracy:Train= 0.5165 Validation= 0.5034\n",
      "Metrics for  1\n",
      "AUROC:Train= 0.5437 Validation= 0.5172 AUPRC:Train= 0.5310 Validation= 0.5125 Accuracy:Train= 0.5303 Validation= 0.5152\n",
      "Metrics for  2\n",
      "AUROC:Train= 0.6210 Validation= 0.5498 AUPRC:Train= 0.5973 Validation= 0.5331 Accuracy:Train= 0.5872 Validation= 0.5112\n",
      "Metrics for  3\n",
      "AUROC:Train= 0.7459 Validation= 0.5029 AUPRC:Train= 0.7244 Validation= 0.5114 Accuracy:Train= 0.6685 Validation= 0.4737\n",
      "======================================================================================================================\n",
      "Epoch 0009 finished!\n",
      "Time= 6.51362\n",
      "Metrics for  DTI\n",
      "AUROC:Train= 0.9896 Validation= 0.8117 AUPRC:Train= 0.9893 Validation= 0.8004 Accuracy:Train= 0.9466 Validation= 0.6226\n",
      "Metrics for  TDI\n",
      "AUROC:Train= 0.9951 Validation= 0.8067 AUPRC:Train= 0.9949 Validation= 0.8361 Accuracy:Train= 0.9545 Validation= 0.6792\n",
      "Metrics for  PPI\n",
      "AUROC:Train= 0.9973 Validation= 0.8503 AUPRC:Train= 0.9968 Validation= 0.8988 Accuracy:Train= 0.9803 Validation= 0.8424\n",
      "Metrics for  0\n",
      "AUROC:Train= 0.5242 Validation= 0.5103 AUPRC:Train= 0.5192 Validation= 0.5109 Accuracy:Train= 0.5163 Validation= 0.5087\n",
      "Metrics for  1\n",
      "AUROC:Train= 0.5518 Validation= 0.5163 AUPRC:Train= 0.5365 Validation= 0.5097 Accuracy:Train= 0.5369 Validation= 0.5083\n",
      "Metrics for  2\n",
      "AUROC:Train= 0.6311 Validation= 0.5399 AUPRC:Train= 0.6040 Validation= 0.5282 Accuracy:Train= 0.5956 Validation= 0.4944\n",
      "Metrics for  3\n",
      "AUROC:Train= 0.7555 Validation= 0.4894 AUPRC:Train= 0.7383 Validation= 0.5215 Accuracy:Train= 0.6760 Validation= 0.4912\n",
      "======================================================================================================================\n",
      "Epoch 0010 finished!\n",
      "Time= 6.52652\n",
      "Metrics for  DTI\n",
      "AUROC:Train= 0.9925 Validation= 0.8291 AUPRC:Train= 0.9923 Validation= 0.8201 Accuracy:Train= 0.9486 Validation= 0.6321\n",
      "Metrics for  TDI\n",
      "AUROC:Train= 0.9951 Validation= 0.8006 AUPRC:Train= 0.9952 Validation= 0.8232 Accuracy:Train= 0.9526 Validation= 0.6415\n",
      "Metrics for  PPI\n",
      "AUROC:Train= 0.9964 Validation= 0.8487 AUPRC:Train= 0.9879 Validation= 0.8963 Accuracy:Train= 0.9803 Validation= 0.8370\n",
      "Metrics for  0\n",
      "AUROC:Train= 0.5297 Validation= 0.5119 AUPRC:Train= 0.5223 Validation= 0.5154 Accuracy:Train= 0.5205 Validation= 0.5098\n",
      "Metrics for  1\n",
      "AUROC:Train= 0.5580 Validation= 0.5189 AUPRC:Train= 0.5427 Validation= 0.5113 Accuracy:Train= 0.5409 Validation= 0.5215\n",
      "Metrics for  2\n",
      "AUROC:Train= 0.6385 Validation= 0.5476 AUPRC:Train= 0.6126 Validation= 0.5323 Accuracy:Train= 0.6016 Validation= 0.5079\n",
      "Metrics for  3\n",
      "AUROC:Train= 0.7673 Validation= 0.5297 AUPRC:Train= 0.7512 Validation= 0.5336 Accuracy:Train= 0.6779 Validation= 0.4912\n"
     ]
    }
   ],
   "source": [
    "# Metric structures initialization\n",
    "val_metrics = np.zeros([FLAGS.epochs,num_edge_types,3])\n",
    "train_metrics = np.zeros([FLAGS.epochs,num_edge_types,3])\n",
    "# Start training\n",
    "print(\"Train model\")\n",
    "for epoch in range(FLAGS.epochs):\n",
    "    t = time.time()\n",
    "    minibatch.shuffle()\n",
    "    itr = 0\n",
    "    while not minibatch.end():\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = minibatch.next_minibatch_feed_dict(placeholders=placeholders)\n",
    "        feed_dict = minibatch.update_feed_dict(\n",
    "            feed_dict=feed_dict,\n",
    "            dropout=FLAGS.dropout,\n",
    "            placeholders=placeholders)\n",
    "        # Training step: run single weight update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.batch_edge_type_idx], feed_dict=feed_dict)\n",
    "        if (itr+1)%1000==0:print('Iteration',itr)\n",
    "        itr += 1\n",
    "    # Train & validation accuracy over all train data per epoch\n",
    "    print('======================================================================================================================')\n",
    "    print(\"Epoch\", \"%04d\" % (epoch + 1),'finished!')\n",
    "    print(\"Time=\", \"{:.5f}\".format(time.time()-t))\n",
    "    for r in range(num_edge_types):\n",
    "        i,j,k = minibatch.idx2edge_type[r]\n",
    "        print('Metrics for ', edge2name[i,j][k])\n",
    "        train_metrics[epoch,r,:] = get_accuracy_scores(\n",
    "            minibatch.train_edges[i,j][k], minibatch.train_edges_false[i,j][k],(i,j,k))\n",
    "        val_metrics[epoch,r,:] = get_accuracy_scores(\n",
    "            minibatch.val_edges[i,j][k], minibatch.val_edges_false[i,j][k],(i,j,k))\n",
    "        print(\"AUROC:Train=\", \"{:.4f}\".format(train_metrics[epoch,r,0])\n",
    "              ,\"Validation=\", \"{:.4f}\".format(val_metrics[epoch,r,0])\n",
    "              ,\"AUPRC:Train=\", \"{:.4f}\".format(train_metrics[epoch,r,1])\n",
    "              ,\"Validation=\", \"{:.4f}\".format(val_metrics[epoch,r,1])\n",
    "              ,\"Accuracy:Train=\", \"{:.4f}\".format(train_metrics[epoch,r,2])\n",
    "              ,\"Validation=\", \"{:.4f}\".format(val_metrics[epoch,r,2]))\n",
    "    output_data['val_metrics'] = val_metrics\n",
    "    output_data['train_metrics'] = train_metrics\n",
    "    output_data['epoch'] = epoch + 1\n",
    "    with open(out_file,'wb') as f:\n",
    "        pickle.dump(output_data, f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of training. Metric structure handling   \n",
    "print(\"Optimization finished!\")\n",
    "test_metrics = np.zeros([num_edge_types,3])\n",
    "for et in range(num_edge_types):\n",
    "    i,j,k = minibatch.idx2edge_type[et]\n",
    "    test_metrics[et,:] = get_accuracy_scores(\n",
    "        minibatch.test_edges[i,j][k], minibatch.test_edges_false[i,j][k], (i,j,k),\n",
    "        noise=bool(noise))\n",
    "    print(\"Edge type=\", edge2name[i,j][k])\n",
    "    print(\"Edge type:\", \"%04d\" % et, \"Test AUROC score\", \"{:.5f}\".format(test_metrics[et,0]))\n",
    "    print(\"Edge type:\", \"%04d\" % et, \"Test AUPRC score\", \"{:.5f}\".format(test_metrics[et,1]))\n",
    "    print(\"Edge type:\", \"%04d\" % et, \"Test Accuracy score\", \"{:.5f}\".format(test_metrics[et,2]))\n",
    "    print()\n",
    "output_data['test_metrics'] = test_metrics\n",
    "memUse = ps.memory_info()\n",
    "print('Virtual memory:', memUse.vms*1e-09,'Gb')\n",
    "print('RSS Memory:', memUse.rss*1e-09,'Gb')\n",
    "train_time=time.time()-pre_train_time\n",
    "output_data['pre_train_time'] = pre_train_time\n",
    "output_data['train_time'] = train_time\n",
    "output_data['edge2name'] = edge2name\n",
    "output_data['drug2idx'] = drug2idx\n",
    "output_data['gene2idx'] = gene2idx\n",
    "output_data['vms'] = memUse.vms\n",
    "output_data['rss'] = memUse.rss\n",
    "with open(out_file,'wb') as f:\n",
    "    pickle.dump(output_data, f, protocol=2)\n",
    "print('Total time:', datetime.timedelta(seconds=time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_dic = {edge_type: [None]*n for edge_type, n in edge_types.items()}\n",
    "for i, j in edge_types:\n",
    "    for k in range(edge_types[i,j]):\n",
    "        et = minibatch.edge_type2idx[i,j,k]\n",
    "        feed_dict.update({placeholders['dropout']: 0})\n",
    "        feed_dict.update({placeholders['batch_edge_type_idx']: et})\n",
    "        feed_dict.update({placeholders['batch_row_edge_type']: i})\n",
    "        feed_dict.update({placeholders['batch_col_edge_type']: j})\n",
    "        opt_dic[i,j][k] = sess.run(opt.predictions, feed_dict=feed_dict)\n",
    "with open('data/data_structures/intento_optimizer','wb') as f:\n",
    "    pickle.dump(opt_dic, f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
